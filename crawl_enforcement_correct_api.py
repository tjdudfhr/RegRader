#!/usr/bin/env python3
import requests
import json
import time
from datetime import datetime
import xml.etree.ElementTree as ET

def crawl_2025_laws_with_correct_enforcement_params():
    """ì˜¬ë°”ë¥¸ API íŒŒë¼ë¯¸í„°ë¡œ ì‹œí–‰ì¼ ê¸°ì¤€ 2025ë…„ ë²•ë ¹ í¬ë¡¤ë§"""
    
    print("ğŸ”§ ì˜¬ë°”ë¥¸ API íŒŒë¼ë¯¸í„°ë¡œ ì‹œí–‰ì¼ ê¸°ì¤€ 2025ë…„ ë²•ë ¹ í¬ë¡¤ë§!")
    print("=" * 60)
    
    api_key = "20241119YCRNECRQT4Q7SHAZE6P5AXRF"
    base_url = "https://www.law.go.kr/DRF/lawService.do"
    
    all_laws = []
    
    # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ì‹œë„
    param_combinations = [
        # ì¡°í•© 1: ê¸°ë³¸ ê²€ìƒ‰ + ì •ë ¬
        {
            'OC': api_key,
            'target': 'law',
            'type': 'XML',
            'display': '100',
            'page': 1,
            'sort': 'enfcDate',  # ì‹œí–‰ì¼ìˆœ ì •ë ¬
            'order': 'desc'      # ë‚´ë¦¼ì°¨ìˆœ
        },
        # ì¡°í•© 2: ì‹œí–‰ì¼ ë²”ìœ„ ì§€ì • (ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ëª… ì‹œë„)
        {
            'OC': api_key,
            'target': 'law',
            'type': 'XML',
            'display': '100',
            'page': 1,
            'enfcDate': '2025',  # 2025ë…„ ì‹œí–‰
        },
        # ì¡°í•© 3: ê²€ìƒ‰ì–´ë¡œ 2025 í¬í•¨
        {
            'OC': api_key,
            'target': 'law',
            'type': 'XML',
            'display': '100',
            'page': 1,
            'query': '2025'
        }
    ]
    
    for i, params in enumerate(param_combinations, 1):
        print(f"\nğŸ§ª íŒŒë¼ë¯¸í„° ì¡°í•© {i} ì‹œë„:")
        for key, value in params.items():
            if key != 'OC':  # API í‚¤ëŠ” ìˆ¨ê¹€
                print(f"   {key}: {value}")
        
        try:
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            content = response.text
            
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì‘ë‹µ ì €ì¥
            debug_file = f'docs/_debug/api_test_combination_{i}.xml'
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"ğŸ“„ ì‘ë‹µ ì €ì¥: {debug_file}")
            
            # HTML ì˜¤ë¥˜ í˜ì´ì§€ì¸ì§€ í™•ì¸
            if content.strip().startswith('<!DOCTYPE html') or '<html' in content[:100]:
                print("âŒ HTML ì˜¤ë¥˜ í˜ì´ì§€ ë°˜í™˜ë¨")
                continue
            
            # XML íŒŒì‹± ì‹œë„
            try:
                root = ET.fromstring(content)
                
                # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ ê²½ë¡œë¡œ ë²•ë ¹ ìš”ì†Œ ì°¾ê¸°
                law_elements = (root.findall('.//law') or 
                              root.findall('.//Law') or 
                              root.findall('.//item') or
                              root.findall('.//LawService'))
                
                if law_elements:
                    print(f"âœ… {len(law_elements)}ê°œ ë²•ë ¹ ìš”ì†Œ ë°œê²¬!")
                    
                    # ì²« ë²ˆì§¸ ìš”ì†Œ êµ¬ì¡° í™•ì¸
                    if len(law_elements) > 0:
                        first_law = law_elements[0]
                        print("ğŸ“‹ ì²« ë²ˆì§¸ ë²•ë ¹ ìš”ì†Œ êµ¬ì¡°:")
                        for child in first_law:
                            print(f"  - {child.tag}: {child.text[:50] if child.text else 'None'}...")
                    
                    # ì´ ì¡°í•©ì´ ì‘ë™í•˜ë¯€ë¡œ ì „ì²´ í¬ë¡¤ë§ ì§„í–‰
                    return crawl_with_working_params(params)
                else:
                    print("âŒ ë²•ë ¹ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
                    # ì „ì²´ XML êµ¬ì¡° í™•ì¸
                    print("ğŸ“‹ XML ë£¨íŠ¸ êµ¬ì¡°:")
                    for child in root:
                        print(f"  - {child.tag}: {child.text[:50] if child.text else 'None'}...")
                
            except ET.ParseError as e:
                print(f"âŒ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
                
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {e}")
            continue
    
    print("\nâŒ ëª¨ë“  íŒŒë¼ë¯¸í„° ì¡°í•©ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    return None

def crawl_with_working_params(base_params):
    """ì‘ë™í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í¬ë¡¤ë§ ìˆ˜í–‰"""
    
    print(f"\nğŸš€ ì‘ë™í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘!")
    
    base_url = "https://www.law.go.kr/DRF/lawService.do"
    all_laws = []
    page = 1
    
    while True:
        params = base_params.copy()
        params['page'] = page
        
        print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        
        try:
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            law_elements = (root.findall('.//law') or 
                          root.findall('.//Law') or 
                          root.findall('.//item') or
                          root.findall('.//LawService'))
            
            if not law_elements:
                print("âŒ ë” ì´ìƒ ë²•ë ¹ì´ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            page_laws = []
            
            for law_elem in law_elements:
                law_data = {}
                
                # ëª¨ë“  í•˜ìœ„ ìš”ì†Œ ìˆ˜ì§‘
                for child in law_elem:
                    if child.text and child.text.strip():
                        law_data[child.tag] = child.text.strip()
                
                # ì‹œí–‰ì¼ì´ 2025ë…„ì¸ ê²ƒë§Œ í•„í„°ë§
                enforcement_fields = ['ì‹œí–‰ì¼ì', 'enfcDate', 'ì‹œí–‰ì¼', 'enforcementDate', 'effDate']
                enforcement_date = None
                
                for field in enforcement_fields:
                    if field in law_data and '2025' in law_data[field]:
                        enforcement_date = law_data[field]
                        break
                
                if not enforcement_date:
                    continue
                
                # ì •ê·œí™”ëœ ë°ì´í„°
                normalized_law = {
                    'title': (law_data.get('ë²•ë ¹ëª…') or 
                             law_data.get('lawNm') or
                             law_data.get('ë²•ë ¹ëª…ì¹­') or ''),
                    'lsId': (law_data.get('ë²•ë ¹ì¼ë ¨ë²ˆí˜¸') or 
                            law_data.get('lsId') or ''),
                    'enforcement_date': enforcement_date,
                    'promulgation_date': (law_data.get('ê³µí¬ì¼ì') or 
                                        law_data.get('promDate') or ''),
                    'ministry': (law_data.get('ì†Œê´€ë¶€ì²˜') or 
                               law_data.get('minstNm') or ''),
                    'law_type': (law_data.get('ë²•ì¢…êµ¬ë¶„') or 
                               law_data.get('lawType') or ''),
                    'raw_data': law_data
                }
                
                if normalized_law['title']:
                    page_laws.append(normalized_law)
            
            all_laws.extend(page_laws)
            print(f"âœ… í˜ì´ì§€ {page}: {len(page_laws)}ê°œ ë²•ë ¹ (2025ë…„ ì‹œí–‰), ì´ {len(all_laws)}ê°œ")
            
            if len(law_elements) < int(base_params.get('display', 100)):
                break
                
            page += 1
            time.sleep(0.5)
            
            if page > 30:  # ì•ˆì „ì¥ì¹˜
                break
                
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            break
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'crawled_at': datetime.now().isoformat(),
        'search_method': 'working_params',
        'total_count': len(all_laws),
        'laws': all_laws
    }
    
    output_file = 'docs/enforcement_2025_laws_correct.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì‹œí–‰ì¼ ê¸°ì¤€ 2025ë…„ ë²•ë ¹ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_laws)}ê°œ")
    print(f"ğŸ“ ì €ì¥: {output_file}")
    
    return result

if __name__ == "__main__":
    crawl_2025_laws_with_correct_enforcement_params()