#!/usr/bin/env python3
"""
êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ 2025ë…„ ì‹œí–‰ì¼ ê¸°ì¤€ ëª¨ë“  ë²•ê·œ í¬ë¡¤ë§
- ì‹œí–‰ì¼ì´ 2025ë…„ì¸ ëª¨ë“  ë²•ë ¹ ìˆ˜ì§‘
- ì •í™•í•œ ì œëª©, ì‹œí–‰ì¼, ë²•ë ¹ID ë“± ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
"""

import requests
import json
import time
from datetime import datetime
import xml.etree.ElementTree as ET

class GovernmentLawCrawler:
    def __init__(self):
        # êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPI ê¸°ë³¸ URL
        self.base_url = "https://www.law.go.kr/DRF/lawSearch.do"
        self.detail_url = "https://www.law.go.kr/DRF/lawService.do"
        
        # API í‚¤ (ê³µê°œ ì„œë¹„ìŠ¤)
        self.params = {
            'OC': 'tjdudfhr',  # ì‚¬ìš©ì ì‹ë³„ì
            'target': 'law',   # ê²€ìƒ‰ ëŒ€ìƒ
            'type': 'XML'      # ì‘ë‹µ í˜•ì‹
        }
        
        self.crawled_laws = []
        
    def crawl_2025_laws(self):
        """2025ë…„ ì‹œí–‰ì¼ ê¸°ì¤€ ë²•ë ¹ í¬ë¡¤ë§"""
        print("ğŸ” 2025ë…„ ì‹œí–‰ ë²•ë ¹ í¬ë¡¤ë§ ì‹œì‘...")
        
        # 2025ë…„ ê° ì›”ë³„ë¡œ ê²€ìƒ‰ (ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´)
        for month in range(1, 13):
            print(f"ğŸ“… 2025ë…„ {month}ì›” ë²•ë ¹ ê²€ìƒ‰ ì¤‘...")
            
            # í•´ë‹¹ ì›”ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
            start_date = f"2025{month:02d}01"
            if month == 12:
                end_date = "20251231"
            else:
                next_month = month + 1
                if next_month <= 12:
                    end_date = f"2025{next_month:02d}01"
                else:
                    end_date = "20251231"
            
            page = 1
            while True:
                # API íŒŒë¼ë¯¸í„° ì„¤ì •
                search_params = self.params.copy()
                search_params.update({
                    'display': '100',  # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜
                    'page': str(page),
                    'search': '3',     # ì‹œí–‰ì¼ì ê²€ìƒ‰
                    'asdt': start_date,  # ì‹œì‘ ì‹œí–‰ì¼
                    'aedt': end_date     # ì¢…ë£Œ ì‹œí–‰ì¼
                })
                
                try:
                    response = requests.get(self.base_url, params=search_params, timeout=30)
                    response.raise_for_status()
                    
                    # XML íŒŒì‹±
                    root = ET.fromstring(response.content)
                    
                    # ì´ ê²°ê³¼ ìˆ˜ í™•ì¸
                    total_count_elem = root.find('.//totalCnt')
                    if total_count_elem is not None:
                        total_count = int(total_count_elem.text)
                        print(f"   - ì´ {total_count}ê±´ ë°œê²¬")
                    
                    # ë²•ë ¹ ì •ë³´ ì¶”ì¶œ
                    laws = root.findall('.//law')
                    if not laws:
                        print(f"   - {month}ì›” {page}í˜ì´ì§€: ê²°ê³¼ ì—†ìŒ")
                        break
                    
                    print(f"   - {month}ì›” {page}í˜ì´ì§€: {len(laws)}ê±´ ì²˜ë¦¬")
                    
                    for law in laws:
                        law_info = self.extract_law_info(law)
                        if law_info and self.is_2025_enforcement(law_info.get('enforcement_date')):
                            # ì¤‘ë³µ ë°©ì§€
                            if not any(existing['ls_id'] == law_info['ls_id'] for existing in self.crawled_laws):
                                self.crawled_laws.append(law_info)
                    
                    # ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                    if len(laws) < 100:
                        break
                    
                    page += 1
                    time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²©
                    
                except requests.exceptions.RequestException as e:
                    print(f"   - API ìš”ì²­ ì˜¤ë¥˜: {e}")
                    time.sleep(2)
                    continue
                except ET.ParseError as e:
                    print(f"   - XML íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
        
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(self.crawled_laws)}ê±´")
        return self.crawled_laws
    
    def extract_law_info(self, law_element):
        """ë²•ë ¹ ì •ë³´ ì¶”ì¶œ"""
        try:
            law_info = {
                'ls_id': self.get_text(law_element, 'lsId'),
                'title': self.get_text(law_element, 'lawNm'),
                'law_type': self.get_text(law_element, 'lawType'),
                'promulgation_date': self.get_text(law_element, 'promDt'),
                'enforcement_date': self.get_text(law_element, 'efctDt'),
                'ministry': self.get_text(law_element, 'admstNm'),
                'law_classification': self.get_text(law_element, 'lawClsCd')
            }
            
            # í•„ìˆ˜ ì •ë³´ í™•ì¸
            if law_info['title'] and law_info['ls_id']:
                return law_info
            else:
                return None
                
        except Exception as e:
            print(f"   - ë²•ë ¹ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def get_text(self, element, tag_name):
        """XML ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        elem = element.find(tag_name)
        return elem.text.strip() if elem is not None and elem.text else ""
    
    def is_2025_enforcement(self, enforcement_date):
        """2025ë…„ ì‹œí–‰ì¼ì¸ì§€ í™•ì¸"""
        if not enforcement_date:
            return False
        return enforcement_date.startswith('2025')
    
    def save_to_file(self, filename='crawled_2025_government_laws.json'):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        data = {
            'crawled_at': datetime.now().isoformat(),
            'total_count': len(self.crawled_laws),
            'description': '2025ë…„ ì‹œí–‰ ì •ë¶€ ë²•ë ¹ (êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°)',
            'source': 'êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPI',
            'laws': self.crawled_laws
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filename}")
        return filename

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = GovernmentLawCrawler()
    
    # 2025ë…„ ë²•ë ¹ í¬ë¡¤ë§
    laws = crawler.crawl_2025_laws()
    
    # ê²°ê³¼ ì €ì¥
    filename = crawler.save_to_file()
    
    # í†µê³„ ì •ë³´ ì¶œë ¥
    print("\nğŸ“Š í¬ë¡¤ë§ í†µê³„:")
    print(f"   - ì´ ë²•ë ¹ ìˆ˜: {len(laws)}ê±´")
    
    # ì›”ë³„ ë¶„í¬
    monthly_count = {}
    for law in laws:
        enforcement_date = law.get('enforcement_date', '')
        if len(enforcement_date) >= 6:
            month = enforcement_date[4:6]
            monthly_count[month] = monthly_count.get(month, 0) + 1
    
    print("   - ì›”ë³„ ë¶„í¬:")
    for month, count in sorted(monthly_count.items()):
        print(f"     {month}ì›”: {count}ê±´")
    
    return filename

if __name__ == "__main__":
    main()