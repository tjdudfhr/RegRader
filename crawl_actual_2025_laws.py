#!/usr/bin/env python3
import os, sys, json, time, hashlib, re, random
import urllib.parse, urllib.request
from datetime import date, datetime
from html import unescape
import xml.etree.ElementTree as ET

# êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPI ì„¤ì •
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) law-watch/3.3"
OPENAPI_BASE = "https://www.law.go.kr/DRF/lawSearch.do"
LAW_RSS = "https://www.law.go.kr/rss/lsRss.do?section=LS"

# OpenAPI í‚¤ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
LAW_OC = os.environ.get("LAW_OC", "knowhow1")

def http_get(url, timeout=45, headers=None, retries=3, backoff=2.0):
    """HTTP GET ìš”ì²­"""
    last = None
    hdr = {"User-Agent": UA, "Accept": "*/*"}
    if headers: hdr.update(headers)
    
    for i in range(retries):
        try:
            req = urllib.request.Request(url, headers=hdr)
            with urllib.request.urlopen(req, timeout=timeout) as r:
                return r.read()
        except Exception as e:
            last = e
            sleep = (backoff ** i) + random.uniform(0, 0.6)
            print(f"[WARN] GET ì‹¤íŒ¨ ({i+1}/{retries}) {url} -> {e}; ì¬ì‹œë„ {sleep:.1f}ì´ˆ í›„", file=sys.stderr)
            time.sleep(sleep)
    
    print(f"[ERROR] GET ìµœì¢… ì‹¤íŒ¨: {url} -> {last}", file=sys.stderr)
    return None

def yyyymmdd_to_iso(s):
    """YYYYMMDD í˜•ì‹ì„ YYYY-MM-DDë¡œ ë³€í™˜"""
    if not s: return None
    s = str(s)
    m = re.search(r"(\d{4})(\d{2})(\d{2})", s)
    if not m: return None
    y, mm, dd = map(int, m.groups())
    return f"{y:04d}-{mm:02d}-{dd:02d}"

def crawl_2025_laws_from_openapi():
    """êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPIì—ì„œ 2025ë…„ ì‹œí–‰ ë²•ë ¹ í¬ë¡¤ë§"""
    
    print("ğŸ” êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPIì—ì„œ 2025ë…„ ì‹œí–‰ ë²•ë ¹ í¬ë¡¤ë§ ì‹œì‘...")
    
    # 2025ë…„ ì „ì²´ ê¸°ê°„
    start_date = "20250101"
    end_date = "20251231"
    
    all_laws = []
    
    # ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    for page in range(1, 20):  # ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€
        print(f"ğŸ“„ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        
        params = {
            "OC": LAW_OC,
            "target": "eflaw",  # ì‹œí–‰ë²•ë ¹
            "type": "JSON",
            "display": "100",   # í˜ì´ì§€ë‹¹ 100ê°œ
            "page": str(page),
            "efYd": f"{start_date}~{end_date}",  # ì‹œí–‰ì¼ì ë²”ìœ„
            "sort": "efdes",    # ì‹œí–‰ì¼ì ë‚´ë¦¼ì°¨ìˆœ
        }
        
        url = OPENAPI_BASE + "?" + urllib.parse.urlencode(params, safe="~:")
        print(f"ğŸŒ ìš”ì²­ URL: {url}")
        
        raw = http_get(url)
        if not raw:
            print(f"âŒ í˜ì´ì§€ {page} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            break
            
        # ë””ë²„ê·¸ìš© ì €ì¥
        os.makedirs("docs/_debug", exist_ok=True)
        with open(f"docs/_debug/openapi_2025_p{page}.json", "wb") as f:
            f.write(raw)
        
        try:
            data = json.loads(raw.decode("utf-8", "ignore"))
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            break
        
        # ë²•ë ¹ ë°ì´í„° ì¶”ì¶œ
        page_laws = extract_laws_from_json(data)
        
        if not page_laws:
            print(f"ğŸ“„ í˜ì´ì§€ {page}ì—ì„œ ë²•ë ¹ ë°ì´í„° ì—†ìŒ, í¬ë¡¤ë§ ì¢…ë£Œ")
            break
            
        all_laws.extend(page_laws)
        print(f"âœ… í˜ì´ì§€ {page}: {len(page_laws)}ê°œ ë²•ë ¹ ìˆ˜ì§‘ (ì´ {len(all_laws)}ê°œ)")
        
        # API ë¶€í•˜ ë°©ì§€
        time.sleep(1)
        
        # 100ê°œ ë¯¸ë§Œì´ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€
        if len(page_laws) < 100:
            print(f"ğŸ“„ í˜ì´ì§€ {page}ê°€ ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤")
            break
    
    print(f"ğŸ‰ ì´ {len(all_laws)}ê°œì˜ 2025ë…„ ì‹œí–‰ ë²•ë ¹ í¬ë¡¤ë§ ì™„ë£Œ!")
    return all_laws

def extract_laws_from_json(data):
    """JSON ë°ì´í„°ì—ì„œ ë²•ë ¹ ì •ë³´ ì¶”ì¶œ"""
    laws = []
    
    def walk_json(obj, path=""):
        """JSON ê°ì²´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ ë²•ë ¹ ë°ì´í„° ì°¾ê¸°"""
        if isinstance(obj, dict):
            # ë²•ë ¹ ì •ë³´ë¥¼ í¬í•¨í•œ ê°ì²´ì¸ì§€ í™•ì¸
            if any(key in obj for key in ["ë²•ë ¹ëª…í•œê¸€", "ë²•ë ¹ëª…", "title", "ì‹œí–‰ì¼ì", "efYd"]):
                laws.append(obj)
            else:
                for key, value in obj.items():
                    walk_json(value, f"{path}.{key}")
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                walk_json(item, f"{path}[{i}]")
    
    walk_json(data)
    
    # ì¶”ì¶œëœ ë²•ë ¹ ë°ì´í„° ì •ì œ
    processed_laws = []
    for raw_law in laws:
        try:
            law = process_law_data(raw_law)
            if law:
                processed_laws.append(law)
        except Exception as e:
            print(f"âš ï¸ ë²•ë ¹ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    return processed_laws

def process_law_data(raw_law):
    """ì›ì‹œ ë²•ë ¹ ë°ì´í„°ë¥¼ ì •ì œëœ í˜•íƒœë¡œ ë³€í™˜"""
    
    # ë²•ë ¹ëª… ì¶”ì¶œ
    title = (raw_law.get("ë²•ë ¹ëª…í•œê¸€") or 
             raw_law.get("ë²•ë ¹ëª…") or 
             raw_law.get("title") or "").strip()
    
    if not title:
        return None
    
    title = unescape(title)
    
    # ì‹œí–‰ì¼ì ì¶”ì¶œ
    effective_date = yyyymmdd_to_iso(
        raw_law.get("ì‹œí–‰ì¼ì") or 
        raw_law.get("ì‹œí–‰ì¼") or 
        raw_law.get("efYd") or ""
    )
    
    if not effective_date or not effective_date.startswith("2025"):
        return None
    
    # ë²•ë ¹ID ì¶”ì¶œ
    law_id = (raw_law.get("ë²•ë ¹ID") or 
              raw_law.get("lsId") or 
              raw_law.get("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸") or "").strip()
    
    # ì†Œê´€ë¶€ì²˜ ì¶”ì¶œ
    ministry = (raw_law.get("ì†Œê´€ë¶€ì²˜ëª…") or 
                raw_law.get("ë¶€ì²˜ëª…") or 
                raw_law.get("ì†Œê´€ë¶€ì²˜") or "").strip()
    
    # ë²•ë ¹ ìœ í˜• ì¶”ì¶œ
    law_type = (raw_law.get("ì œê°œì •êµ¬ë¶„ëª…") or 
                raw_law.get("êµ¬ë¶„") or 
                raw_law.get("ì œê°œì •êµ¬ë¶„") or "").strip()
    
    # ê°œì • ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    if not law_type:
        if any(keyword in title for keyword in ["ê°œì •", "ì‹ ì„¤", "ì œì •", "ì „ë¶€ê°œì •", "ì¼ë¶€ê°œì •"]):
            law_type = "ê°œì •"
        else:
            law_type = "ì‹œí–‰"
    
    # URL ìƒì„±
    detail_url = f"https://www.law.go.kr/LSW/lsInfoP.do?lsId={law_id}" if law_id else ""
    search_url = "https://www.law.go.kr/lsSc.do?query=" + urllib.parse.quote(title)
    
    # ê³ ìœ  ID ìƒì„±
    key = title + detail_url
    unique_id = hashlib.md5(key.encode("utf-8")).hexdigest()
    
    law_data = {
        "id": unique_id,
        "title": title,
        "effectiveDate": effective_date,
        "lawType": law_type,
        "ministry": ministry,
        "lsId": law_id,
        "detailUrl": detail_url,
        "searchUrl": search_url,
        "source": "êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPI",
        "crawled_at": datetime.now().isoformat()
    }
    
    return law_data

def save_crawled_data(laws):
    """í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥"""
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    unique_laws = {}
    for law in laws:
        title = law["title"]
        if title not in unique_laws:
            unique_laws[title] = law
        else:
            # ë” ìì„¸í•œ ì •ë³´ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ì—…ë°ì´íŠ¸
            if law.get("lsId") and not unique_laws[title].get("lsId"):
                unique_laws[title] = law
    
    final_laws = list(unique_laws.values())
    
    # ì‹œí–‰ì¼ì ê¸°ì¤€ ì •ë ¬
    final_laws.sort(key=lambda x: (x.get("effectiveDate", ""), x.get("title", "")))
    
    # ë¶„ì„ ì •ë³´
    analysis = {
        "crawled_at": datetime.now().isoformat(),
        "total_laws": len(final_laws),
        "source": "êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° OpenAPI",
        "year": 2025,
        "date_range": "2025-01-01 ~ 2025-12-31",
        "laws": final_laws
    }
    
    # ë¶„ê¸°ë³„ í†µê³„
    quarters = {"Q1": 0, "Q2": 0, "Q3": 0, "Q4": 0}
    for law in final_laws:
        date_str = law.get("effectiveDate", "")
        if len(date_str) >= 7:
            month = int(date_str[5:7])
            if 1 <= month <= 3:
                quarters["Q1"] += 1
            elif 4 <= month <= 6:
                quarters["Q2"] += 1
            elif 7 <= month <= 9:
                quarters["Q3"] += 1
            elif 10 <= month <= 12:
                quarters["Q4"] += 1
    
    analysis["by_quarter"] = quarters
    
    # ë¶€ì²˜ë³„ í†µê³„
    ministries = {}
    for law in final_laws:
        ministry = law.get("ministry", "ë¯¸ìƒ")
        ministries[ministry] = ministries.get(ministry, 0) + 1
    
    analysis["by_ministry"] = ministries
    
    # ì €ì¥
    output_file = "docs/crawled_2025_laws.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š ì´ ë²•ë ¹ ìˆ˜: {len(final_laws)}ê°œ")
    print(f"ğŸ“… ë¶„ê¸°ë³„ ë¶„í¬: {quarters}")
    print(f"ğŸ›ï¸ ìƒìœ„ ë¶€ì²˜: {dict(list(sorted(ministries.items(), key=lambda x: x[1], reverse=True))[:5])}")
    
    return analysis

def main():
    print("ğŸš€ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° 2025ë…„ ì‹œí–‰ ë²•ë ¹ ì •í™•í•œ í¬ë¡¤ë§ ì‹œì‘!")
    print("=" * 60)
    
    # 1. OpenAPIì—ì„œ í¬ë¡¤ë§
    laws = crawl_2025_laws_from_openapi()
    
    if not laws:
        print("âŒ í¬ë¡¤ë§ëœ ë²•ë ¹ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # 2. ë°ì´í„° ì €ì¥ ë° ë¶„ì„
    analysis = save_crawled_data(laws)
    
    print("=" * 60)
    print("âœ… 2025ë…„ ì‹¤ì œ ì‹œí–‰ ë²•ë ¹ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: docs/crawled_2025_laws.json")
    
    return analysis

if __name__ == "__main__":
    main()